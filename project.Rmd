---
title: "Practical Machine Learning"
output: html_document
---

This is a document for Practical Machine Learning Course Project. The goal of this project is to make predictions on Weight Lifting Exercise Dataset (http://groupware.les.inf.puc-rio.br/har).

First of all, we should load the data set. Remember to download the dataset into your working directory if you want to rerun the codes.

```{r,cache=TRUE}
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
```

I decide to ignore features, which are meaningless or hard to be used such as X(id), cvtd_timestamp.
```{r}
training <- training[-c(1,3,4,5)]
testing <- testing[-c(1,3,4,5)]
```

Then, we load the required libraries. In this project, I use caret package for cross validatation data splitting and RWeka for training random forest classifiers. I also try to use random forest model in caret but it is too slow.
```{r,warning=FALSE}
library(caret)
if (Sys.getenv("JAVA_HOME")!="")
  Sys.setenv(JAVA_HOME="")
library(rJava)
library(RWeka)
# make random forest classifier
RF <- make_Weka_classifier("weka/classifiers/trees/RandomForest")
```

We split the training data into 10 folds for cross validation.
```{r,warning=FALSE}
folds = 10
CV <- createFolds(training$classe,k=folds)
```

We then traing our model by 10-folds cross validation and illustrates the confusion matrix and accuracy on both of training and validation in each folds. Before training the random forest model, we perfrom several pre-processing. We first transform most of variable to numeric, and then remove the fields with all NAs. Further, we impute the missing values by the mean of each field. Note that we only use the sub-train each folds to decide the imputed value and apply this on both of sub-train and validation. It seems the random forest model perform very well in all of the 10 folds of data. The accuracy is above 0.99 on all of the folds.

```{r,warning=FALSE,cache=TRUE}
totalErr = 0
for (i in 1:10){
  todelete <- c()
  print(c("fold",i))
  parTrain <- training[-CV[[i]],]
  parValid <- training[CV[[i]],]
  # transform numeric variable from factor to numeric, imputing data and remove the fields with all NA 
  for(j in 1:(ncol(parTrain)-1)){
    if(class(parTrain[[j]])=="factor"){
      parTrain[[j]] <- as.character(parTrain[[j]])
      parValid[[j]] <- as.character(parValid[[j]])
    }
    parTrain[[j]] <- as.numeric(parTrain[[j]])
    parValid[[j]] <- as.numeric(parValid[[j]])
    
    if(sum(!is.na(parTrain[j]))==0){
      todelete <- append(todelete,j)
    }
    # We impute the missing values my mean of the fields
    # note that the imputed value is decided by parTrain only.
    toimpute <- mean(parTrain[[j]],na.rm = T)
    parTrain[[j]][is.na(parTrain[[j]])] <-toimpute
    parValid[[j]][is.na(parValid[[j]])] <-toimpute

  }
  # removethe fields with all NA 
  if(length(todelete)!=0){
    parTrain <- parTrain[-todelete]
    parValid <- parValid[-todelete]
  }

  # train random forest model
  model <- RF(classe~.,data=parTrain,control = Weka_control(I = 10,K=0,S=1))
  # show confusing matrix and accuracy on training and validation set for each fold
  print('training:')
  pre <- predict(model,parTrain)
  print(table(pre,parTrain$classe))
  TrainAcc <- sum(pre==parTrain$classe)/length(parTrain$classe)
  print(TrainAcc)
  print('validation:')
  pre <- predict(model,parValid)
  print(table(pre,parValid$classe))
  validAcc <- sum(pre==parValid$classe)/length(parValid$classe)
  print(validAcc)  
  totalErr = totalErr + sum(pre==parValid$classe)
}
```

The estimated out of sample error by cross validation is:
```{r,echo=FALSE}
  Err = totalErr/nrow(training)
  print(Err)
```


Finally, we can make prediction on the testing data. follow the procedure above, we rerun the same steps on the whole training data and make predictions on the testing data. The predicted labels for the testing instances are as following:

```{r,warning=FALSE,cache=TRUE,echo=FALSE}
  
  todelete <- c()
  parTrain <- training
  parValid <- testing
  # transform factor variable as numeric, imputing data and remove the fields with all NA 
  for(j in 1:(ncol(parTrain)-1)){
    if(class(parTrain[[j]])=="factor"){
      parTrain[[j]] <- as.character(parTrain[[j]])
      parValid[[j]] <- as.character(parValid[[j]])
    }
    parTrain[[j]] <- as.numeric(parTrain[[j]])
    parValid[[j]] <- as.numeric(parValid[[j]])
    
    if(sum(!is.na(parTrain[j]))==0){
      todelete <- append(todelete,j)
    }
    # We impute the missing values my mean of the fields
    # note that the imputed value is decided by parTrain only.
    toimpute <- mean(parTrain[[j]],na.rm = T)
    parTrain[[j]][is.na(parTrain[[j]])] <-toimpute
    parValid[[j]][is.na(parValid[[j]])] <-toimpute

  }
  # removethe fields with all NA 
  if(length(todelete)!=0){
    parTrain <- parTrain[-todelete]
    parValid <- parValid[-todelete]
  }

  # train random forest model
  model <- RF(classe~.,data=parTrain,control = Weka_control(I = 10,K=0,S=1))
  # show confusing matrix and accuracy on training and validation set for each fold
  pre <- predict(model,parValid)
  print(pre)
```

we write the predictions into files by following codes. Submitting to the course website, the accuracy of this model is 100% on the testing data.
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(as.character(pre))

```